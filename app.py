import os
import torch
import streamlit as st
from PIL import Image
from diffusers import StableDiffusionPipeline, AutoencoderKL, UNet2DConditionModel, DDPMScheduler
from transformers import CLIPTextModel, CLIPTokenizer
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# --- Configuration ---
MODEL_NAME = "runwayml/stable-diffusion-v1-5"
OUTPUT_MODEL_DIR = "output_lora_model"
FORENSIC_TOKEN = "<forensic_details>"
LORA_WEIGHTS_FILENAME = "adapter_model.safetensors"
LORA_WEIGHTS_PATH = os.path.join(OUTPUT_MODEL_DIR, LORA_WEIGHTS_FILENAME)
MIXED_PRECISION = "fp16"
GUIDANCE_SCALE = 7.5
NUM_INFERENCE_STEPS = 30
SEED = 1234

@st.cache_resource
def load_pipeline():
    from diffusers import StableDiffusionPipeline, UNet2DConditionModel, AutoencoderKL, DDPMScheduler
    from transformers import CLIPTextModel, CLIPTokenizer

    tokenizer = CLIPTokenizer.from_pretrained(MODEL_NAME, subfolder="tokenizer")
    text_encoder = CLIPTextModel.from_pretrained(MODEL_NAME, subfolder="text_encoder")
    vae = AutoencoderKL.from_pretrained(MODEL_NAME, subfolder="vae")
    unet = UNet2DConditionModel.from_pretrained(MODEL_NAME, subfolder="unet")
    scheduler = DDPMScheduler.from_pretrained(MODEL_NAME, subfolder="scheduler")

    tokenizer.add_tokens(FORENSIC_TOKEN)
    text_encoder.resize_token_embeddings(len(tokenizer))

    pipe = StableDiffusionPipeline(
        vae=vae,
        text_encoder=text_encoder,
        tokenizer=tokenizer,
        unet=unet,
        scheduler=scheduler,
        safety_checker=None,
        feature_extractor=None,
        requires_safety_checker=False
    )

    pipe.load_lora_weights("output_lora_model/adapter_model.safetensors")
    pipe.to("cuda" if torch.cuda.is_available() else "cpu")
    return pipe

# --- Streamlit UI --- 
import streamlit as st

# Constants
FORENSIC_TOKEN = "[FORENSIC]"
NUM_INFERENCE_STEPS = 50
GUIDANCE_SCALE = 7.5

# Custom CSS for better styling
st.markdown("""
<style>
    .main-header {
        text-align: center;
        padding: 2rem 0;
        background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
        background-clip: text;
        font-size: 3rem;
        font-weight: bold;
        margin-bottom: 1rem;
    }
    
    .subtitle {
        text-align: center;
        color: #666;
        font-size: 1.2rem;
        margin-bottom: 2rem;
    }
    
    .section-header {
        background-color: #f0f2f6;
        padding: 0.5rem 1rem;
        border-radius: 0.5rem;
        border-left: 4px solid #667eea;
        margin: 1rem 0;
    }
    
    .tip-box {
        background-color: #e8f4fd;
        padding: 1rem;
        border-radius: 0.5rem;
        border-left: 4px solid #1f77b4;
        margin: 1rem 0;
    }
    
    .settings-box {
        background-color: #fff3cd;
        padding: 1rem;
        border-radius: 0.5rem;
        border-left: 4px solid #ffc107;
        margin: 1rem 0;
    }
    
    .stButton > button {
        background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);
        color: white;
        border: none;
        padding: 0.75rem 2rem;
        border-radius: 0.5rem;
        font-weight: bold;
        font-size: 1.1rem;
        width: 100%;
        transition: all 0.3s ease;
    }
    
    .stButton > button:hover {
        transform: translateY(-2px);
        box-shadow: 0 4px 12px rgba(102, 126, 234, 0.4);
    }
</style>
""", unsafe_allow_html=True)

# Main header
st.markdown('<h1 class="main-header">üé≠ Forensic Face Generator</h1>', unsafe_allow_html=True)
st.markdown('<p class="subtitle">Enter a detailed forensic description to generate a realistic face image using advanced AI technology</p>', unsafe_allow_html=True)

# Create main layout with columns
col1, col2 = st.columns([2, 1])

with col1:
    # Main input section
    st.markdown('<div class="section-header"><h3>üìù Forensic Description</h3></div>', unsafe_allow_html=True)
    
    prompt_input = st.text_area(
        "Detailed Description",
        value=f"a forensic portrait of {FORENSIC_TOKEN} a young female with high cheekbones, wearing lipstick and earrings",
        height=120,
        help="Be as detailed as possible for better results. Include features like age, gender, facial structure, and accessories."
    )
    
    # Negative prompt in an expander for cleaner look
    with st.expander("üö´ Negative Prompt (Advanced)", expanded=False):
        negative_prompt = st.text_input(
            "What to avoid in the generation",
            value="blurry, low quality, cartoon, sketch",
            help="Specify what you want to exclude from the generated image"
        )

with col2:
    # Tips section
    st.markdown("""
    <div class="tip-box">
        <h4>üí° Pro Tips</h4>
        <ul>
            <li>Include specific facial features and proportions</li>
            <li>Mention age range and gender for better accuracy</li>
            <li>Add details about hair, eyes, and skin tone</li>
            <li>Describe clothing and accessories if relevant</li>
        </ul>
    </div>
    """, unsafe_allow_html=True)

# Advanced settings section
st.markdown('<div class="section-header"><h3>‚öôÔ∏è Generation Settings</h3></div>', unsafe_allow_html=True)

# Create two columns for sliders
slider_col1, slider_col2 = st.columns(2)

with slider_col1:
    st.markdown("**üîÑ Inference Steps**")
    num_steps = st.slider(
        "Steps",
        min_value=10,
        max_value=100,
        value=NUM_INFERENCE_STEPS,
        help="Higher values produce more detailed results but take longer to generate"
    )
    st.caption(f"Current: {num_steps} steps")

with slider_col2:
    st.markdown("**üéØ Guidance Scale**")
    guidance = st.slider(
        "Guidance",
        min_value=1.0,
        max_value=15.0,
        value=GUIDANCE_SCALE,
        step=0.1,
        help="Controls how closely the AI follows your prompt. Higher values mean stricter adherence"
    )
    st.caption(f"Current: {guidance:.1f}")

# Settings guide
st.markdown("""
<div class="settings-box">
    <h4>‚öôÔ∏è Settings Guide</h4>
    <ul>
        <li><strong>Steps:</strong> 20-50 for quick results, 50-100 for high quality</li>
        <li><strong>Guidance:</strong> 5-10 for creative freedom, 10-15 for precise control</li>
        <li><strong>Negative Prompts:</strong> Use to avoid unwanted elements like blur or cartoonish features</li>
    </ul>
</div>
""", unsafe_allow_html=True)

# Generate button with custom styling
st.markdown("---")
generate_button = st.button("üöÄ Generate Forensic Portrait", type="primary")

# Display current settings summary
if st.checkbox("üìä Show Generation Summary", value=False):
    st.info(f"""
    **Current Settings:**
    - **Prompt:** {prompt_input[:100]}{'...' if len(prompt_input) > 100 else ''}
    - **Negative Prompt:** {negative_prompt}
    - **Inference Steps:** {num_steps}
    - **Guidance Scale:** {guidance}
    """)

# Add some spacing and footer
st.markdown("---")
st.markdown("""
<div style="text-align: center; color: #666; padding: 1rem;">
    <small>üî¨ Advanced AI-powered forensic portrait generation ‚Ä¢ Built with Streamlit</small>
</div>
""", unsafe_allow_html=True)

# Handle generation (you can add your generation logic here)
if generate_button:
    with st.spinner("üé® Generating forensic portrait..."):
        # Your generation logic here
        st.success("‚úÖ Generation parameters configured successfully!")
        st.balloons()
        
        # Display the parameters that would be used
        st.json({
            "prompt": prompt_input,
            "negative_prompt": negative_prompt,
            "num_inference_steps": num_steps,
            "guidance_scale": guidance
        })
if generate_button and prompt_input:
    pipe = load_pipeline()
    with torch.no_grad():
        generator = torch.Generator(device=pipe.device).manual_seed(SEED)
        result = pipe(
            prompt=prompt_input,
            negative_prompt=negative_prompt,
            num_inference_steps=num_steps,
            guidance_scale=guidance,
            num_images_per_prompt=1,
            generator=generator,
        )
        image = result.images[0]
        st.image(image, caption="Generated Forensic Face", use_container_width=True)
        output_path = os.path.join("generated_images", "streamlit_output.png")
        os.makedirs("generated_images", exist_ok=True)
        image.save(output_path)
        st.success(f"Image saved to {output_path}")